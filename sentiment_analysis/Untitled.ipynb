{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c5bdcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'my', 'first', 'text.', \"Let's\", 'start']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is my first text. Let's start\"\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e7efef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "word_tokenized = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cdf6e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is my first text.', \"Let's start\"]\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d6af2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('This is my first text.', 1), (\"Let's start\", 1)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "Frequency_distribution = FreqDist(word_tokenized)\n",
    "\n",
    "print(Frequency_distribution.most_common(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a6ace2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.22833333333333333\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "with open('mytxtneg.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment.polarity\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9fd6983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Having the option to deal with awful days is a particularly significant expertise to acquire throughout everyday life, since awful days are inescapable, and in the event that they're not taken care of usefully, they can turn from awful \"days\", into weeks and even months.\n",
      "Regularly, you may neglect to perceive when you're having an awful day, or you may even attempt to keep away from your miserable sentiments or imagine they don't exist.\n",
      "A decent method to comfort yourself is to say, \"This is truly hard for you at the present time.\n",
      "Regularly on awful days you may feel like you can't complete things and battle to be beneficial, which frequently leaves you feeling baffled and futile.\n",
      "This makes it difficult to tell how to react to an awful day.\n"
     ]
    }
   ],
   "source": [
    "from newspaper import Article\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "url1 = 'https://aornjournal.onlinelibrary.wiley.com/doi/10.1002/aorn.13479'\n",
    "url2 = 'https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3652533/'\n",
    "url3 = 'https://www.voicesofyouth.org/blog/bad-days-are-okay'\n",
    "\n",
    "article = Article(url3)\n",
    "article.download()\n",
    "article.parse()\n",
    "article.nlp()\n",
    "\n",
    "text = article.summary\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71593896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.40882352941176475\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment.polarity\n",
    "\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ddd84a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.22, 'neu': 0.675, 'pos': 0.105, 'compound': -0.9586}\n"
     ]
    }
   ],
   "source": [
    "print(analyzer.polarity_scores(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b34a0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens:  ['This', 'first', 'text', '.', 'Let', \"'s\", 'start']\n",
      "removed tokens:  ['is', 'my']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tesfaye/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "text = \"This is my first text. Let's start\"\n",
    "\n",
    "tokenizedword = word_tokenize(text)\n",
    "\n",
    "tokenwithoutstopwords = []\n",
    "removed = []\n",
    "\n",
    "for word in tokenizedword:\n",
    "    if word not in stop_words:\n",
    "        tokenwithoutstopwords.append(word)\n",
    "    else:\n",
    "        removed.append(word)\n",
    "\n",
    "print('tokens: ', tokenwithoutstopwords)\n",
    "print('removed tokens: ', removed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
